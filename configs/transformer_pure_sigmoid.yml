global_parameters:
  # model related
  - &model_arch_type 'TransformersModel'
  - &is_train true #[true,false] # 如果不使用transformers model，该参数表示是否训练词向量，如果使用transformers model，该参数表示是否对transformers model 进行微调
  - &class_num 1  #
  # data related
  - &dataset_type 'DisasterDataset'
  - &data_dir 'data/train_valid_test/'
  - &cache_dir 'data/.cache'
  - &overwrite_cache true   # 是否覆盖已经处理好的数据缓存。当数据处理耗费时间较长时，通过配置该参数为false，可再下次加载数据时，直接加载处理好的数据缓存。从而节省不必要的时间浪费
  - &transformer_model 'roberta-base' # 详情参见： https://huggingface.co/models [roberta-base,prajjwal1/bert-tiny,'clue/albert_chinese_tiny','voidful/albert_chinese_small','voidful/albert_chinese_base','hfl/chinese-bert-wwm','']
  - &force_download false
  - &num_workers 16
  - &batch_size 16



experiment_name: *model_arch_type
num_gpu: 2                         # GPU数量
main_device_id: '3'
device_id: '3'
visual_device: '3'
resume_path: null                         # path to latest checkpoint
seed: 12345
k_fold: 5  # 0 表示不使用交叉验证

# 模型
model_arch:
  type: *model_arch_type
  args:
    transformer_model: *transformer_model
    cache_dir: *cache_dir
    force_download: *force_download
    is_train: *is_train
    dropout: 0.1
    class_num: *class_num

all_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'all.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker

train_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'train.jsonl'  # [train_.jsonl, train_random.jsonl]
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker

valid_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'valid.jsonl' # [valid_.jsonl, valid_random.jsonl]
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: false
    batch_size: *batch_size  # data loader batch size
    num_workers: *num_workers # data loader num of worker

test_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'test.jsonl' #
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: false
    batch_size: *batch_size  # data loader batch size
    num_workers: *num_workers # data loader num of worker

optimizer:
  type: 'AdamW'
  transformers_lr: 2e-5
  crf_lr: 1e-3
  fc_lr: 1e-3
  weight_decay: 1e-2
  args:
    amsgrad: false

lr_scheduler:
  type: 'get_linear_schedule_with_warmup'
  warmup_proportion: 0.1
  args:
    last_epoch: -1


loss:
  loss_cut: 0  # 防止过拟合
  losses:
    - "binary_loss"    # ['ce_loss','focal_loss','label_smoothing_ce_loss']

metrics:
  - "binary_accuracy"

trainer:
  epochs: 3
  save_dir: 'saved/'
  save_period: 10
  verbosity: 2
  monitor: "max val_binary_accuracy"
  early_stop: 3
  tensorboard: true
  add_graph: false   # 是否把模型结构图添加到tensorboard
  clip_grad: true
  max_grad_norm: 1.0
  is_adversarial_training: false

adversarial_training:
  type: 'FGM'
  args:
    emb_name: 'word_embeddings'
    epsilon: 1.0
    alpha: 0.3